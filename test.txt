import scrapy
import json
from car.car.items import LaptopItem

class AsusLaptopsSpider(scrapy.Spider):
    """
    Spider to scrape ASUS laptops from Digikala.
    
    To scrape other brands, you can:
    1. Create similar spiders by replacing 'asus' in the URL with other brands like:
       - 'lenovo': /categories/notebook-netbook-ultrabook/brands/lenovo/search/
       - 'hp': /categories/notebook-netbook-ultrabook/brands/hp/search/
       - 'dell': /categories/notebook-netbook-ultrabook/brands/dell/search/
       - 'acer': /categories/notebook-netbook-ultrabook/brands/acer/search/
    2. Or make this spider more generic by accepting brand name as an argument
    """
    
    name = "asus_laptops"
    allowed_domains = [
        "api.digikala.com",
        "digikala.com",
        "www.digikala.com",
        ]
    start_urls = [
        "https://api.digikala.com/v1/categories/notebook-netbook-ultrabook/brands/asus/search/?page=1",
    ]

    async def parse(self, response):
        data = json.loads(response.body)
        products = data.get('data', {}).get('products', [])
        
        for product in products:
            try:
                laptop = LaptopItem()
                laptop['title'] = product.get('title_fa', '')
                laptop['price'] = product.get('default_variant', {}).get('price', {}).get('selling_price', '')
                laptop['brand'] = 'ASUS'
                laptop['model'] = product.get('title_en', '')
                laptop['specs'] = product.get('specifications', {})
                laptop['image_url'] = product.get('image', {}).get('url', '')
                laptop['source_url'] = f"https://digikala.com/product/{product.get('id')}"
                laptop['year'] = product.get('year', '2000/01/01')
                laptop['extra_data'] = product.get('extra_data', {})
                yield laptop
            except Exception as e:
                custom_log(f"asus_laptopspy:\n\n{e}")

        # Get current page number from the URL
        current_page = int(response.url.split('page=')[1])
        
        # Check if there are more products
        if products:
            custom_log(f"Scraped page {current_page}")
            # Construct URL for next page
            next_page = f"https://api.digikala.com/v1/categories/notebook-netbook-ultrabook/brands/asus/search/?page={current_page + 1}"
            custom_log(f"Next page URL: {next_page}")
            yield scrapy.Request(next_page, callback=self.parse)
            custom_log(f"Parsed next page URL: {next_page}")

def custom_log(value):
    print("##################################################")
    print("asus_laptops.py:\n\n", value)
    print("##################################################")
